From 3f5786b7e8d53dc596828e9cf8cac9ff49ba1676 Mon Sep 17 00:00:00 2001
From: Brett Mastbergen <bmastbergen@untangle.com>
Date: Mon, 13 Jan 2020 16:34:15 -0500
Subject: [PATCH 8/8] untangle: imq 4.16 patch

This allows ingress shaping and shaping over multiple interfaces.

Patch retreived from:
https://github.com/imq/linuximq/blob/master/kernel/v4.x/linux-4.16-imq.diff
---
 drivers/net/Kconfig              | 119 +++++++++++++++++++++++++++++++
 drivers/net/Makefile             |   1 +
 include/linux/netdevice.h        |  18 +++++
 include/linux/skbuff.h           |  22 ++++++
 include/net/netfilter/nf_queue.h |   6 ++
 include/net/pkt_sched.h          |   2 +
 include/net/sch_generic.h        |   7 ++
 include/uapi/linux/netfilter.h   |   3 +-
 net/core/dev.c                   |   9 +++
 net/core/skbuff.c                | 107 +++++++++++++++++++++++++++
 net/netfilter/Kconfig            |  12 ++++
 net/netfilter/Makefile           |   1 +
 net/netfilter/core.c             |   5 ++
 net/netfilter/nf_queue.c         |  43 ++++++++++-
 net/sched/sch_generic.c          |   8 +++
 15 files changed, 360 insertions(+), 3 deletions(-)

diff --git a/drivers/net/Kconfig b/drivers/net/Kconfig
index 600b9d0..7d3e55a 100644
--- a/drivers/net/Kconfig
+++ b/drivers/net/Kconfig
@@ -339,6 +339,125 @@ config RIONET_RX_SIZE
 	depends on RIONET
 	default "128"
 
+config IMQ
+	tristate "IMQ (intermediate queueing device) support"
+	depends on NETDEVICES && NETFILTER
+	---help---
+	  The IMQ device(s) is used as placeholder for QoS queueing
+	  disciplines. Every packet entering/leaving the IP stack can be
+	  directed through the IMQ device where it's enqueued/dequeued to the
+	  attached qdisc. This allows you to treat network devices as classes
+	  and distribute bandwidth among them. Iptables is used to specify
+	  through which IMQ device, if any, packets travel.
+
+	  More information at: https://github.com/imq/linuximq
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called imq.  If unsure, say N.
+
+choice
+	prompt "IMQ behavior (PRE/POSTROUTING)"
+	depends on IMQ
+	default IMQ_BEHAVIOR_AB
+	help
+	  This setting defines how IMQ behaves in respect to its
+	  hooking in PREROUTING and POSTROUTING.
+
+	  IMQ can work in any of the following ways:
+
+	      PREROUTING   |      POSTROUTING
+	  -----------------|-------------------
+	  #1  After NAT    |      After NAT
+	  #2  After NAT    |      Before NAT
+	  #3  Before NAT   |      After NAT
+	  #4  Before NAT   |      Before NAT
+
+	  The default behavior is to hook before NAT on PREROUTING
+	  and after NAT on POSTROUTING (#3).
+
+	  This settings are specially usefull when trying to use IMQ
+	  to shape NATed clients.
+
+	  More information can be found at: https://github.com/imq/linuximq
+
+	  If not sure leave the default settings alone.
+
+config IMQ_BEHAVIOR_AA
+	bool "IMQ AA"
+	help
+	  This setting defines how IMQ behaves in respect to its
+	  hooking in PREROUTING and POSTROUTING.
+
+	  Choosing this option will make IMQ hook like this:
+
+	  PREROUTING:   After NAT
+	  POSTROUTING:  After NAT
+
+	  More information can be found at: https://github.com/imq/linuximq
+
+	  If not sure leave the default settings alone.
+
+config IMQ_BEHAVIOR_AB
+	bool "IMQ AB"
+	help
+	  This setting defines how IMQ behaves in respect to its
+	  hooking in PREROUTING and POSTROUTING.
+
+	  Choosing this option will make IMQ hook like this:
+
+	  PREROUTING:   After NAT
+	  POSTROUTING:  Before NAT
+
+	  More information can be found at: https://github.com/imq/linuximq
+
+	  If not sure leave the default settings alone.
+
+config IMQ_BEHAVIOR_BA
+	bool "IMQ BA"
+	help
+	  This setting defines how IMQ behaves in respect to its
+	  hooking in PREROUTING and POSTROUTING.
+
+	  Choosing this option will make IMQ hook like this:
+
+	  PREROUTING:   Before NAT
+	  POSTROUTING:  After NAT
+
+	  More information can be found at: https://github.com/imq/linuximq
+
+	  If not sure leave the default settings alone.
+
+config IMQ_BEHAVIOR_BB
+	bool "IMQ BB"
+	help
+	  This setting defines how IMQ behaves in respect to its
+	  hooking in PREROUTING and POSTROUTING.
+
+	  Choosing this option will make IMQ hook like this:
+
+	  PREROUTING:   Before NAT
+	  POSTROUTING:  Before NAT
+
+	  More information can be found at: https://github.com/imq/linuximq
+
+	  If not sure leave the default settings alone.
+
+endchoice
+
+config IMQ_NUM_DEVS
+	int "Number of IMQ devices"
+	range 2 16
+	depends on IMQ
+	default "16"
+	help
+	  This setting defines how many IMQ devices will be created.
+
+	  The default value is 16.
+
+	  More information can be found at: https://github.com/imq/linuximq
+
+	  If not sure leave the default settings alone.
+
 config TUN
 	tristate "Universal TUN/TAP device driver support"
 	depends on INET
diff --git a/drivers/net/Makefile b/drivers/net/Makefile
index 72e18d5..f3454bb 100644
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -14,6 +14,7 @@ obj-$(CONFIG_WIREGUARD) += wireguard/
 obj-$(CONFIG_EQUALIZER) += eql.o
 obj-$(CONFIG_IFB) += ifb.o
 obj-$(CONFIG_MACSEC) += macsec.o
+obj-$(CONFIG_IMQ) += imq.o
 obj-$(CONFIG_MACVLAN) += macvlan.o
 obj-$(CONFIG_MACVTAP) += macvtap.o
 obj-$(CONFIG_MII) += mii.o
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 4fdeccf..1c75915 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -2028,6 +2028,11 @@ struct net_device {
 /*
  * Cache lines mostly used on receive path (including eth_type_trans())
  */
+
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+	unsigned long		last_rx;
+#endif
+
 	/* Interface address info used in eth_type_trans() */
 	unsigned char		*dev_addr;
 
@@ -4345,6 +4350,19 @@ static inline void netif_tx_unlock_bh(struct net_device *dev)
 	}						\
 }
 
+#define HARD_TX_LOCK_BH(dev, txq) {           \
+    if ((dev->features & NETIF_F_LLTX) == 0) {  \
+        __netif_tx_lock_bh(txq);      \
+    }                       \
+}
+
+#define HARD_TX_UNLOCK_BH(dev, txq) {          \
+    if ((dev->features & NETIF_F_LLTX) == 0) {  \
+        __netif_tx_unlock_bh(txq);         \
+    }                       \
+}
+
+
 static inline void netif_tx_disable(struct net_device *dev)
 {
 	unsigned int i;
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index acbf187..d724af8 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -37,6 +37,9 @@
 #include <linux/in6.h>
 #include <linux/if_packet.h>
 #include <net/flow.h>
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+#include <linux/imq.h>
+#endif
 #if IS_ENABLED(CONFIG_NF_CONNTRACK)
 #include <linux/netfilter/nf_conntrack_common.h>
 #endif
@@ -746,6 +749,9 @@ struct sk_buff {
 	 * first. This is owned by whoever has the skb queued ATM.
 	 */
 	char			cb[48] __aligned(8);
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+	void			*cb_next;
+#endif
 
 	union {
 		struct {
@@ -757,6 +763,9 @@ struct sk_buff {
 
 #if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)
 	unsigned long		 _nfct;
+#endif
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+        struct nf_queue_entry   *nf_queue_entry;
 #endif
 	unsigned int		len,
 				data_len;
@@ -847,6 +856,9 @@ struct sk_buff {
 	__u8			offload_fwd_mark:1;
 	__u8			offload_l3_fwd_mark:1;
 #endif
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+	__u8			imq_flags:IMQ_F_BITS;
+#endif
 #ifdef CONFIG_NET_CLS_ACT
 	__u8			tc_skip_classify:1;
 	__u8			tc_at_ingress:1;
@@ -1068,6 +1080,12 @@ static inline void consume_skb(struct sk_buff *skb)
 
 void __consume_stateless_skb(struct sk_buff *skb);
 void  __kfree_skb(struct sk_buff *skb);
+
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+int skb_save_cb(struct sk_buff *skb);
+int skb_restore_cb(struct sk_buff *skb);
+#endif
+
 extern struct kmem_cache *skbuff_head_cache;
 
 void kfree_skb_partial(struct sk_buff *skb, bool head_stolen);
@@ -4284,6 +4302,10 @@ static inline void __nf_copy(struct sk_buff *dst, const struct sk_buff *src,
 	dst->_nfct = src->_nfct;
 	nf_conntrack_get(skb_nfct(src));
 #endif
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+	dst->imq_flags = src->imq_flags;
+	dst->nf_queue_entry = src->nf_queue_entry;
+#endif
 #if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)
 	if (copy)
 		dst->nf_trace = src->nf_trace;
diff --git a/include/net/netfilter/nf_queue.h b/include/net/netfilter/nf_queue.h
index e770bba..91f1cd1 100644
--- a/include/net/netfilter/nf_queue.h
+++ b/include/net/netfilter/nf_queue.h
@@ -36,6 +36,12 @@ struct nf_queue_handler {
 void nf_register_queue_handler(struct net *net, const struct nf_queue_handler *qh);
 void nf_unregister_queue_handler(struct net *net);
 void nf_reinject(struct nf_queue_entry *entry, unsigned int verdict);
+void nf_queue_entry_release_refs(struct nf_queue_entry *entry);
+
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+void nf_register_queue_imq_handler(const struct nf_queue_handler *qh);
+void nf_unregister_queue_imq_handler(void);
+#endif
 
 void nf_queue_entry_get_refs(struct nf_queue_entry *entry);
 void nf_queue_entry_free(struct nf_queue_entry *entry);
diff --git a/include/net/pkt_sched.h b/include/net/pkt_sched.h
index 2be90a5..d428143 100644
--- a/include/net/pkt_sched.h
+++ b/include/net/pkt_sched.h
@@ -120,6 +120,8 @@ bool sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,
 
 void __qdisc_run(struct Qdisc *q);
 
+struct sk_buff *qdisc_dequeue_skb(struct Qdisc *q, bool *validate);
+
 static inline void qdisc_run(struct Qdisc *q)
 {
 	if (qdisc_run_begin(q)) {
diff --git a/include/net/sch_generic.h b/include/net/sch_generic.h
index f8631ad..c17f3a6 100644
--- a/include/net/sch_generic.h
+++ b/include/net/sch_generic.h
@@ -145,6 +145,13 @@ static inline bool qdisc_is_running(struct Qdisc *qdisc)
 	return (raw_read_seqcount(&qdisc->running) & 1) ? true : false;
 }
 
+static inline int qdisc_enqueue_root(struct sk_buff *skb, struct Qdisc *sch,
+				      struct sk_buff **to_free)
+{
+    qdisc_skb_cb(skb)->pkt_len = skb->len;
+    return qdisc_enqueue(skb, sch, to_free) & NET_XMIT_MASK;
+}
+
 static inline bool qdisc_is_percpu_stats(const struct Qdisc *q)
 {
 	return q->flags & TCQ_F_CPUSTATS;
diff --git a/include/uapi/linux/netfilter.h b/include/uapi/linux/netfilter.h
index ef9a442..6d4b8b8 100644
--- a/include/uapi/linux/netfilter.h
+++ b/include/uapi/linux/netfilter.h
@@ -14,7 +14,8 @@
 #define NF_QUEUE 3
 #define NF_REPEAT 4
 #define NF_STOP 5	/* Deprecated, for userspace nf_queue compatibility. */
-#define NF_MAX_VERDICT NF_STOP
+#define NF_IMQ_QUEUE 6
+#define NF_MAX_VERDICT NF_IMQ_QUEUE
 
 /* we overload the higher bits for encoding auxiliary data such as the queue
  * number or errno values. Not nice, but better than additional function
diff --git a/net/core/dev.c b/net/core/dev.c
index b9d19fb..4448fa3 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -139,6 +139,9 @@
 #include <linux/hrtimer.h>
 #include <linux/netfilter_ingress.h>
 #include <linux/crash_dump.h>
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+#include <linux/imq.h>
+#endif
 #include <linux/sctp.h>
 #include <net/udp_tunnel.h>
 #include <linux/net_namespace.h>
@@ -3567,7 +3570,11 @@ static int xmit_one(struct sk_buff *skb, struct net_device *dev,
 	unsigned int len;
 	int rc;
 
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+	if ((dev_nit_active(dev)) && !(skb->imq_flags & IMQ_F_ENQUEUE))
+#else
 	if (dev_nit_active(dev))
+#endif
 		dev_queue_xmit_nit(skb, dev);
 
 	len = skb->len;
@@ -3607,6 +3614,8 @@ struct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *de
 	return skb;
 }
 
+EXPORT_SYMBOL_GPL(dev_hard_start_xmit);
+
 static struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,
 					  netdev_features_t features)
 {
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 825e6b9..19c6859 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -86,6 +86,87 @@ static struct kmem_cache *skbuff_ext_cache __ro_after_init;
 #endif
 int sysctl_max_skb_frags __read_mostly = MAX_SKB_FRAGS;
 EXPORT_SYMBOL(sysctl_max_skb_frags);
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+static struct kmem_cache *skbuff_cb_store_cache __read_mostly;
+#endif
+
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+/* Control buffer save/restore for IMQ devices */
+struct skb_cb_table {
+	char			cb[48] __aligned(8);
+	void			*cb_next;
+	atomic_t		refcnt;
+};
+
+static DEFINE_SPINLOCK(skb_cb_store_lock);
+
+int skb_save_cb(struct sk_buff *skb)
+{
+	struct skb_cb_table *next;
+
+	next = kmem_cache_alloc(skbuff_cb_store_cache, GFP_ATOMIC);
+	if (!next)
+		return -ENOMEM;
+
+	BUILD_BUG_ON(sizeof(skb->cb) != sizeof(next->cb));
+
+	memcpy(next->cb, skb->cb, sizeof(skb->cb));
+	next->cb_next = skb->cb_next;
+
+	atomic_set(&next->refcnt, 1);
+
+	skb->cb_next = next;
+	return 0;
+}
+EXPORT_SYMBOL(skb_save_cb);
+
+int skb_restore_cb(struct sk_buff *skb)
+{
+	struct skb_cb_table *next;
+
+	if (!skb->cb_next)
+		return 0;
+
+	next = skb->cb_next;
+
+	BUILD_BUG_ON(sizeof(skb->cb) != sizeof(next->cb));
+
+	memcpy(skb->cb, next->cb, sizeof(skb->cb));
+	skb->cb_next = next->cb_next;
+
+	spin_lock(&skb_cb_store_lock);
+
+	if (atomic_dec_and_test(&next->refcnt))
+		kmem_cache_free(skbuff_cb_store_cache, next);
+
+	spin_unlock(&skb_cb_store_lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(skb_restore_cb);
+
+static void skb_copy_stored_cb(struct sk_buff *   , const struct sk_buff *     ) __attribute__ ((unused));
+static void skb_copy_stored_cb(struct sk_buff *new, const struct sk_buff *__old)
+{
+	struct skb_cb_table *next;
+	struct sk_buff *old;
+
+	if (!__old->cb_next) {
+		new->cb_next = NULL;
+		return;
+	}
+
+	spin_lock(&skb_cb_store_lock);
+
+	old = (struct sk_buff *)__old;
+
+	next = old->cb_next;
+	atomic_inc(&next->refcnt);
+	new->cb_next = next;
+
+	spin_unlock(&skb_cb_store_lock);
+}
+#endif
 
 /**
  *	skb_panic - private function for out-of-line support
@@ -665,6 +746,28 @@ void skb_release_head_state(struct sk_buff *skb)
 		WARN_ON(in_irq());
 		skb->destructor(skb);
 	}
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+	/*
+	 * This should not happen. When it does, avoid memleak by restoring
+	 * the chain of cb-backups.
+	 */
+	while (skb->cb_next != NULL) {
+		if (net_ratelimit())
+			pr_warn("IMQ: kfree_skb: skb->cb_next: %08x\n",
+				(unsigned int)(uintptr_t)skb->cb_next);
+
+		skb_restore_cb(skb);
+	}
+	/*
+	 * This should not happen either, nf_queue_entry is nullified in
+	 * imq_dev_xmit(). If we have non-NULL nf_queue_entry then we are
+	 * leaking entry pointers, maybe memory. We don't know if this is
+	 * pointer to already freed memory, or should this be freed.
+	 * If this happens we need to add refcounting, etc for nf_queue_entry.
+	 */
+	if (skb->nf_queue_entry && net_ratelimit())
+		pr_warn("%s\n", "IMQ: kfree_skb: skb->nf_queue_entry != NULL");
+#endif
 #if IS_ENABLED(CONFIG_NF_CONNTRACK)
 	nf_conntrack_put(skb_nfct(skb));
 #endif
@@ -944,6 +1047,10 @@ static void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)
 	skb_dst_copy(new, old);
 	__skb_ext_copy(new, old);
 	__nf_copy(new, old, false);
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+	new->cb_next = NULL;
+	/*skb_copy_stored_cb(new, old);*/
+#endif
 
 	/* Note : this field could be in headers_start/headers_end section
 	 * It is not yet because we do not want to have a 16 bit hole
diff --git a/net/netfilter/Kconfig b/net/netfilter/Kconfig
index 5237021..f18f2ff 100644
--- a/net/netfilter/Kconfig
+++ b/net/netfilter/Kconfig
@@ -922,6 +922,18 @@ config NETFILTER_XT_TARGET_LOG
 
 	  To compile it as a module, choose M here.  If unsure, say N.
 
+config NETFILTER_XT_TARGET_IMQ
+        tristate '"IMQ" target support'
+	depends on NETFILTER_XTABLES
+	depends on IP_NF_MANGLE || IP6_NF_MANGLE
+	select IMQ
+	default m if NETFILTER_ADVANCED=n
+        help
+          This option adds a `IMQ' target which is used to specify if and
+          to which imq device packets should get enqueued/dequeued.
+
+          To compile it as a module, choose M here.  If unsure, say N.
+
 config NETFILTER_XT_TARGET_MARK
 	tristate '"MARK" target support'
 	depends on NETFILTER_ADVANCED
diff --git a/net/netfilter/Makefile b/net/netfilter/Makefile
index 0e0ded8..030a48e 100644
--- a/net/netfilter/Makefile
+++ b/net/netfilter/Makefile
@@ -147,6 +147,7 @@ obj-$(CONFIG_NETFILTER_XT_TARGET_CT) += xt_CT.o
 obj-$(CONFIG_NETFILTER_XT_TARGET_DSCP) += xt_DSCP.o
 obj-$(CONFIG_NETFILTER_XT_TARGET_HL) += xt_HL.o
 obj-$(CONFIG_NETFILTER_XT_TARGET_HMARK) += xt_HMARK.o
+obj-$(CONFIG_NETFILTER_XT_TARGET_IMQ) += xt_IMQ.o
 obj-$(CONFIG_NETFILTER_XT_TARGET_LED) += xt_LED.o
 obj-$(CONFIG_NETFILTER_XT_TARGET_LOG) += xt_LOG.o
 obj-$(CONFIG_NETFILTER_XT_TARGET_NETMAP) += xt_NETMAP.o
diff --git a/net/netfilter/core.c b/net/netfilter/core.c
index 63d0321..926347f 100644
--- a/net/netfilter/core.c
+++ b/net/netfilter/core.c
@@ -596,6 +596,11 @@ int nf_hook_slow(struct sk_buff *skb, struct nf_hook_state *state,
 			if (ret == 0)
 				ret = -EPERM;
 			return ret;
+		case NF_IMQ_QUEUE:
+			ret = nf_queue(skb, state, e, s, verdict);
+			if (ret == -ECANCELED)
+				continue;
+			return ret;
 		case NF_QUEUE:
 			ret = nf_queue(skb, state, s, verdict);
 			if (ret == 1)
diff --git a/net/netfilter/nf_queue.c b/net/netfilter/nf_queue.c
index bbd1209..4e480d3 100644
--- a/net/netfilter/nf_queue.c
+++ b/net/netfilter/nf_queue.c
@@ -29,6 +29,23 @@
  * receives, no matter what.
  */
 
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+static const struct nf_queue_handler __rcu *queue_imq_handler __read_mostly;
+
+void nf_register_queue_imq_handler(const struct nf_queue_handler *qh)
+{
+	rcu_assign_pointer(queue_imq_handler, qh);
+}
+EXPORT_SYMBOL_GPL(nf_register_queue_imq_handler);
+
+void nf_unregister_queue_imq_handler(void)
+{
+	RCU_INIT_POINTER(queue_imq_handler, NULL);
+	synchronize_rcu();
+}
+EXPORT_SYMBOL_GPL(nf_unregister_queue_imq_handler);
+#endif
+
 /* return EBUSY when somebody else is registered, return EEXIST if the
  * same handler is registered, return 0 in case of success. */
 void nf_register_queue_handler(struct net *net, const struct nf_queue_handler *qh)
@@ -153,16 +170,30 @@ static void nf_ip6_saveroute(const struct sk_buff *skb,
 }
 
 static int __nf_queue(struct sk_buff *skb, const struct nf_hook_state *state,
-		      unsigned int index, unsigned int queuenum)
+		      unsigned int index, unsigned int verdict)
 {
 	struct nf_queue_entry *entry = NULL;
 	const struct nf_queue_handler *qh;
 	struct net *net = state->net;
 	unsigned int route_key_size;
+	unsigned int queuetype = verdict & NF_VERDICT_MASK;
+	unsigned int queuenum  = verdict >> NF_VERDICT_QBITS;
 	int status;
 
 	/* QUEUE == DROP if no one is waiting, to be safe. */
 	qh = rcu_dereference(net->nf.queue_handler);
+
+	if (queuetype == NF_IMQ_QUEUE) {
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+	qh = rcu_dereference(queue_imq_handler);
+#else
+	BUG();
+	goto err_unlock;
+#endif
+	} else {
+		qh = rcu_dereference(net->nf.queue_handler);
+	}
+
 	if (!qh)
 		return -ESRCH;
 
@@ -222,8 +253,16 @@ int nf_queue(struct sk_buff *skb, struct nf_hook_state *state,
 {
 	int ret;
 
-	ret = __nf_queue(skb, state, index, verdict >> NF_VERDICT_QBITS);
+	ret = __nf_queue(skb, state, index, verdict);
 	if (ret < 0) {
+
+#if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
+/* IMQ Bypass */
+	if (ret == -ECANCELED && skb->imq_flags == 0) {
+		return 1;
+	}
+#endif
+
 		if (ret == -ESRCH &&
 		    (verdict & NF_VERDICT_FLAG_QUEUE_BYPASS))
 			return 1;
diff --git a/net/sched/sch_generic.c b/net/sched/sch_generic.c
index 05aa257..67220c8 100644
--- a/net/sched/sch_generic.c
+++ b/net/sched/sch_generic.c
@@ -296,6 +296,14 @@ static struct sk_buff *dequeue_skb(struct Qdisc *q, bool *validate,
 	return skb;
 }
 
+struct sk_buff *qdisc_dequeue_skb(struct Qdisc *q, bool *validate)
+{
+	int packets;
+
+	return dequeue_skb(q, validate, &packets);
+}
+EXPORT_SYMBOL(qdisc_dequeue_skb);
+
 /*
  * Transmit possibly several skbs, and handle the return status as
  * required. Owning running seqcount bit guarantees that
-- 
2.32.0

